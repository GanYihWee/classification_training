{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce0d5a22-f754-43c8-b1ee-68e06579bd16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'12.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "from earlystopping import EarlyStopping\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "torch.version.cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "939db4ed-c06a-4b5b-b8d3-99dff8d768e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from self_transformers import Albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e791764-4bfa-4830-a8bc-0b716a69e812",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        Albumentations(),\n",
    "        transforms.Resize((32, 256)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.ConvertImageDtype(torch.float)\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize((32, 256)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.ConvertImageDtype(torch.float)\n",
    "        \n",
    "    ]),\n",
    "}\n",
    "\n",
    "data_dir = '/data/yihwee/MinorTampering/3d_tampering/train_test_val_4'\n",
    "\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'val']}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=32, shuffle=True, num_workers=6) for x in ['train', 'val']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95d2cd2d-0b6c-45a8-8b57-446b2127f2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(model, weights = None):\n",
    "    \n",
    "    model = model(weights = weights) if weights != None else model()\n",
    "\n",
    "    #Finetune Final few layers to adjust for tiny imagenet input\n",
    "    model.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "    num_ftrs = model.fc.in_features\n",
    "\n",
    "    model.fc = nn.Sequential(nn.Linear(num_ftrs, 1), nn.Sigmoid())\n",
    "    model.to(device)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d974fbf8-23e6-4b34-a9c7-5dbc584d97c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yihwee/anaconda3/envs/p311-cu121/lib/python3.11/site-packages/torch/cuda/__init__.py:126: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11060). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#Load Resnet18\n",
    "album_rgb_resnet18 = create_model(resnet18, ResNet18_Weights.IMAGENET1K_V1)\n",
    "\n",
    "#Following is the loss function and optimization used for baseline model\n",
    "#Loss Function\n",
    "loss_fn = nn.BCELoss()\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer = optim.SGD(album_rgb_resnet18.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47a3e886-b02c-4fa7-913a-97191302c223",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=1, bias=True)\n",
       "    (1): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "album_rgb_resnet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8d2f13e-05cf-407a-95de-058afcd62971",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_losses = []\n",
    "results = []\n",
    "def test(dataloader, model, loss_fn):\n",
    "    \n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device, dtype=torch.float)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y.reshape(-1,1)).item()\n",
    "            preds = [1 if i > 0.5 else 0 for i in pred]\n",
    "            label = [1 if i > 0.5 else 0 for i in y.reshape(-1,1)]\n",
    "            correct += sum([1 if preds[i] == label[i] else 0 for i in range(0, len(preds))])\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    test_losses.append(test_loss)\n",
    "            \n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    \n",
    "    return test_loss, model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "faca96a3-49bb-4a8d-8a10-aab72b06706a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "\n",
    "        X, y = X.to(device), y.to(device, dtype=torch.float)\n",
    "\n",
    "        # Compute prediction error\n",
    "        output = model(X)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = loss_fn(output, y.reshape(-1,1))\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "            train_losses.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "866475c4-b30e-4dda-b3e2-0c50834fbc23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.663953  [    0/16877]\n",
      "loss: 0.261193  [ 3200/16877]\n",
      "loss: 0.244756  [ 6400/16877]\n",
      "loss: 0.225757  [ 9600/16877]\n",
      "loss: 0.167743  [12800/16877]\n",
      "loss: 0.151114  [16000/16877]\n",
      "Test Error: \n",
      " Accuracy: 97.3%, Avg loss: 0.084451 \n",
      "\n",
      "Validation loss decreased (inf --> 0.084451).  Saving model ...\n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.123514  [    0/16877]\n",
      "loss: 0.395871  [ 3200/16877]\n",
      "loss: 0.033564  [ 6400/16877]\n",
      "loss: 0.042028  [ 9600/16877]\n",
      "loss: 0.203128  [12800/16877]\n",
      "loss: 0.034042  [16000/16877]\n",
      "Test Error: \n",
      " Accuracy: 98.2%, Avg loss: 0.066828 \n",
      "\n",
      "Validation loss decreased (0.084451 --> 0.066828).  Saving model ...\n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.031083  [    0/16877]\n",
      "loss: 0.214073  [ 3200/16877]\n",
      "loss: 0.025618  [ 6400/16877]\n",
      "loss: 0.145475  [ 9600/16877]\n",
      "loss: 0.194133  [12800/16877]\n",
      "loss: 0.136224  [16000/16877]\n",
      "Test Error: \n",
      " Accuracy: 98.6%, Avg loss: 0.050110 \n",
      "\n",
      "Validation loss decreased (0.066828 --> 0.050110).  Saving model ...\n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.088797  [    0/16877]\n",
      "loss: 0.125934  [ 3200/16877]\n",
      "loss: 0.025123  [ 6400/16877]\n",
      "loss: 0.030507  [ 9600/16877]\n",
      "loss: 0.015716  [12800/16877]\n",
      "loss: 0.167484  [16000/16877]\n",
      "Test Error: \n",
      " Accuracy: 98.8%, Avg loss: 0.049348 \n",
      "\n",
      "Validation loss decreased (0.050110 --> 0.049348).  Saving model ...\n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.026397  [    0/16877]\n",
      "loss: 0.063195  [ 3200/16877]\n",
      "loss: 0.016040  [ 6400/16877]\n",
      "loss: 0.167156  [ 9600/16877]\n",
      "loss: 0.205130  [12800/16877]\n",
      "loss: 0.059080  [16000/16877]\n",
      "Test Error: \n",
      " Accuracy: 98.8%, Avg loss: 0.039281 \n",
      "\n",
      "Validation loss decreased (0.049348 --> 0.039281).  Saving model ...\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "early_stopping = EarlyStopping(patience=5, verbose=True, path='test')\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(dataloaders['train'], album_rgb_resnet18, loss_fn, optimizer)\n",
    "    test_loss, model_chkpnt = test(dataloaders['val'], album_rgb_resnet18, loss_fn)\n",
    "    \n",
    "    #Earlystopping\n",
    "    early_stopping(test_loss, model_chkpnt) \n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "        \n",
    "    # Release memory\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d124d10d-2930-4ee4-9150-e3c836a83b89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load latest checkpoint\n",
    "final_model = create_model(resnet18, ResNet18_Weights.IMAGENET1K_V1)\n",
    "final_model.load_state_dict(torch.load('checkpoints/test_checkpoint.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e257ecdb-217a-43a9-9178-41c747cffe82",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms2 = {\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize((32, 256)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.ConvertImageDtype(torch.float)\n",
    "    ]),\n",
    "}\n",
    "\n",
    "image_datasets2 = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms2[x]) for x in ['test']}\n",
    "dataset_sizes2 = {x: len(image_datasets2[x]) for x in ['test']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ca178de-0572-4e41-be78-71d337f6f13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = []\n",
    "actual = []\n",
    "\n",
    "for i in range(0, dataset_sizes2['test']):\n",
    "    x, y = image_datasets2['test'][i][0], image_datasets2['test'][i][1]\n",
    "    x=x.to(device)\n",
    "    final_model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred = final_model(x.unsqueeze(0))\n",
    "        predicted.append([1 if x > 0.8 else 0 for x in pred][0])\n",
    "        actual.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "927a17c9-457c-4836-8485-a0eb3a6932fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99      2294\n",
      "           1       1.00      0.98      0.99      2281\n",
      "\n",
      "    accuracy                           0.99      4575\n",
      "   macro avg       0.99      0.99      0.99      4575\n",
      "weighted avg       0.99      0.99      0.99      4575\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(actual, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7cc14c2-bc65-4493-8976-61b1de9a2f0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Generate the confusion matrix for test set\n",
    "cf_matrix = confusion_matrix(actual, predicted)\n",
    "\n",
    "ax = sns.heatmap(cf_matrix, annot=True, cmap='Blues', fmt='g')\n",
    "\n",
    "ax.set_title('Confusion Matrix with labels for Test Data\\n\\n');\n",
    "ax.set_xlabel('\\nPredicted Values')\n",
    "ax.set_ylabel('Actual Values ');\n",
    "\n",
    "## Ticket labels - List must be in alphabetical order\n",
    "ax.xaxis.set_ticklabels(['False','True'])\n",
    "ax.yaxis.set_ticklabels(['False','True'])\n",
    "\n",
    "## Display the visualization of the Confusion Matrix.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ab66936b-db17-4121-9d68-d2f3752e768b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(final_model, 'models/album_rgb_resnet18.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p311-cu121",
   "language": "python",
   "name": "p311-cu121"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
